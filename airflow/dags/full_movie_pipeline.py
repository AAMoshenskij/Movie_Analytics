from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime, timedelta
import subprocess
import time
import requests

default_args = {
    'owner': 'data_engineer',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2)
}

def check_services_health():
    """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ Ð²ÑÐµÑ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²"""
    print("ðŸ” ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÑ ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²...")
    
    services = {
        'Kafka': ('kafka', 9092),  # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¸Ð¼Ñ ÑÐµÑ€Ð²Ð¸ÑÐ° Ð²Ð¼ÐµÑÑ‚Ð¾ localhost
        'PostgreSQL': ('postgres', 5432),
        'Spark Master': ('spark-master', 8080),
        'Zookeeper': ('zookeeper', 2181)
    }
    
    for service, (host, port) in services.items():
        try:
            import socket
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(10)  # Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚
            result = sock.connect_ex((host, port))
            sock.close()
            
            if result == 0:
                print(f"âœ… {service} Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ Ð½Ð° {host}:{port}")
            else:
                print(f"âŒ {service} Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ Ð½Ð° {host}:{port}")
                # ÐÐµ Ð¿Ð°Ð´Ð°ÐµÐ¼ ÑÑ€Ð°Ð·Ñƒ, Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÑƒ Ð´Ñ€ÑƒÐ³Ð¸Ñ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²
                if service == 'Kafka':
                    raise Exception(f"{service} Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½")
                
        except Exception as e:
            print(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ {service}: {e}")
            if service == 'Kafka':
                raise


def start_kafka_producer():
    """Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Kafka producer Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
    print("ðŸš€ Ð—Ð°Ð¿ÑƒÑÐº Kafka producer...")
    
    try:
        # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð²Ð°Ñˆ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ…
        process = subprocess.Popen([
            'python3', '/opt/airflow/scripts/kafka_producer_fixed.py'
        ])
        
        # Ð”Ð°ÐµÐ¼ Ð²Ñ€ÐµÐ¼Ñ Ð½Ð° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÑŽ Ð´Ð°Ð½Ð½Ñ‹Ñ…
        print("â³ Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Kafka...")
        time.sleep(30)  # Ð–Ð´ÐµÐ¼ 30 ÑÐµÐºÑƒÐ½Ð´
        
        # Ð—Ð°Ð²ÐµÑ€ÑˆÐ°ÐµÐ¼ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ
        process.terminate()
        process.wait()
        
        print("âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ð² Kafka")
        
    except Exception as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð¿ÑƒÑÐºÐ° producer: {e}")
        raise

def start_spark_streaming():
    """Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Spark Streaming Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Kafka"""
    print("ðŸ”¥ Ð—Ð°Ð¿ÑƒÑÐº Spark Streaming...")
    
    try:
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ð½Ðµ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½ Ð»Ð¸ ÑƒÐ¶Ðµ Spark
        try:
            with open('/tmp/spark_streaming.pid', 'r') as f:
                old_pid = int(f.read().strip())
                import os
                try:
                    os.kill(old_pid, 0)  # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°
                    print(f"âš ï¸ Spark ÑƒÐ¶Ðµ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½ Ñ PID {old_pid}, Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼...")
                    os.kill(old_pid, 9)  # ÐŸÑ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð·Ð°Ð²ÐµÑ€ÑˆÐ°ÐµÐ¼
                    time.sleep(5)
                except OSError:
                    pass  # ÐŸÑ€Ð¾Ñ†ÐµÑÑÐ° Ð½ÐµÑ‚, Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼
        except FileNotFoundError:
            pass
        
        # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Spark Streaming Ð² Ñ„Ð¾Ð½Ð¾Ð²Ð¾Ð¼ Ñ€ÐµÐ¶Ð¸Ð¼Ðµ
        spark_process = subprocess.Popen([
            'python3', '/opt/airflow/scripts/spark_streaming_postgres.py'
        ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ PID Ð´Ð»Ñ Ð¿Ð¾ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ³Ð¾ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ
        with open('/tmp/spark_streaming.pid', 'w') as f:
            f.write(str(spark_process.pid))
        
        # Ð”Ð°ÐµÐ¼ Ð²Ñ€ÐµÐ¼Ñ Ð½Ð° Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÑŽ
        print("â³ ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Spark...")
        time.sleep(30)
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ ÐµÑ‰Ðµ Ð¶Ð¸Ð²
        if spark_process.poll() is not None:
            stdout, stderr = spark_process.communicate()
            print(f"âŒ Spark Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð·Ð°Ð²ÐµÑ€ÑˆÐ¸Ð»ÑÑ Ñ ÐºÐ¾Ð´Ð¾Ð¼ {spark_process.returncode}")
            print(f"STDERR: {stderr.decode()}")
            raise Exception("Spark Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð½Ðµ Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ð»ÑÑ")
        
        print("âœ… Spark Streaming Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½ Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ")
        
    except Exception as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð¿ÑƒÑÐºÐ° Spark: {e}")
        raise


def stop_spark_streaming():
    """ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÑ‚ Spark Streaming"""
    print("ðŸ›‘ ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Spark Streaming...")
    
    try:
        # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ PID Ð¸ Ð¾ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ
        with open('/tmp/spark_streaming.pid', 'r') as f:
            pid = int(f.read().strip())
        
        import os
        import signal
        os.kill(pid, signal.SIGTERM)
        
        print("âœ… Spark Streaming Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½")
        
    except Exception as e:
        print(f"âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Spark: {e}")

def verify_processed_data():
    """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚, Ñ‡Ñ‚Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ñ‹ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð² PostgreSQL"""
    print("ðŸ“Š ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
    
    try:
        hook = PostgresHook(postgres_conn_id='analytics_db')
        conn = hook.get_conn()
        cursor = conn.cursor()
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ñ… Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð°Ñ…
        tables_to_check = ['user_views_processed', 'movie_stats_realtime', 'device_stats_realtime']
        
        for table in tables_to_check:
            cursor.execute(f"SELECT COUNT(*) FROM {table}")
            count = cursor.fetchone()[0]
            print(f"ðŸ“ˆ Ð¢Ð°Ð±Ð»Ð¸Ñ†Ð° {table}: {count} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
            
            if count == 0:
                raise Exception(f"Ð¢Ð°Ð±Ð»Ð¸Ñ†Ð° {table} Ð¿ÑƒÑÑ‚Ð°!")
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÐ²ÐµÐ¶ÐµÑÑ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ñ…
        cursor.execute("""
            SELECT MAX(processing_timestamp) as latest_data 
            FROM user_views_processed
        """)
        latest_timestamp = cursor.fetchone()[0]
        print(f"ðŸ•’ ÐŸÐ¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ: {latest_timestamp}")
        
        cursor.close()
        conn.close()
        
        print("âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ñ‹ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹")
        
    except Exception as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {e}")
        raise

def update_business_metrics():
    """ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÑ‚ Ð±Ð¸Ð·Ð½ÐµÑ-Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
    print("ðŸ’¼ ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð±Ð¸Ð·Ð½ÐµÑ-Ð¼ÐµÑ‚Ñ€Ð¸Ðº...")
    
    try:
        hook = PostgresHook(postgres_conn_id='analytics_db')
        conn = hook.get_conn()
        cursor = conn.cursor()
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ Ð´Ð»Ñ Ð±Ð¸Ð·Ð½ÐµÑ-Ð¼ÐµÑ‚Ñ€Ð¸Ðº
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS business_metrics (
                metric_date DATE PRIMARY KEY,
                total_views INTEGER,
                unique_users INTEGER,
                total_watch_time INTEGER,
                popular_movie VARCHAR(100),
                avg_session_duration FLOAT,
                calculated_at TIMESTAMP DEFAULT NOW()
            )
        """)
        
        # Ð Ð°ÑÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
        cursor.execute("""
            INSERT INTO business_metrics (
                metric_date, total_views, unique_users, total_watch_time, 
                popular_movie, avg_session_duration
            )
            SELECT 
                CURRENT_DATE as metric_date,
                COUNT(*) as total_views,
                COUNT(DISTINCT user_id) as unique_users,
                SUM(duration_seconds) as total_watch_time,
                (SELECT movie_id FROM user_views_processed 
                 GROUP BY movie_id ORDER BY COUNT(*) DESC LIMIT 1) as popular_movie,
                AVG(duration_seconds) as avg_session_duration
            FROM user_views_processed 
            WHERE DATE(processing_timestamp) = CURRENT_DATE
            ON CONFLICT (metric_date) DO UPDATE SET
                total_views = EXCLUDED.total_views,
                unique_users = EXCLUDED.unique_users,
                total_watch_time = EXCLUDED.total_watch_time,
                popular_movie = EXCLUDED.popular_movie,
                avg_session_duration = EXCLUDED.avg_session_duration,
                calculated_at = NOW()
        """)
        
        conn.commit()
        
        # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
        cursor.execute("SELECT * FROM business_metrics ORDER BY metric_date DESC LIMIT 1")
        latest_metrics = cursor.fetchone()
        
        if latest_metrics:
            print("ðŸ“ˆ ÐŸÐ¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ Ð±Ð¸Ð·Ð½ÐµÑ-Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸:")
            print(f"   - Ð”Ð°Ñ‚Ð°: {latest_metrics[0]}")
            print(f"   - ÐŸÑ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ñ‹: {latest_metrics[1]}")
            print(f"   - Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸: {latest_metrics[2]}")
            print(f"   - ÐžÐ±Ñ‰ÐµÐµ Ð²Ñ€ÐµÐ¼Ñ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð°: {latest_metrics[3]} ÑÐµÐº")
            print(f"   - ÐŸÐ¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ð¹ Ñ„Ð¸Ð»ÑŒÐ¼: {latest_metrics[4]}")
            print(f"   - Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ: {latest_metrics[5]:.2f} ÑÐµÐº")
        
        cursor.close()
        conn.close()
        
        print("âœ… Ð‘Ð¸Ð·Ð½ÐµÑ-Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹")
        
    except Exception as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð¼ÐµÑ‚Ñ€Ð¸Ðº: {e}")
        raise

def cleanup_temp_data():
    """ÐžÑ‡Ð¸Ñ‰Ð°ÐµÑ‚ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ"""
    print("ðŸ§¹ ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
    
    try:
        # Ð£Ð´Ð°Ð»ÑÐµÐ¼ PID Ñ„Ð°Ð¹Ð»
        import os
        if os.path.exists('/tmp/spark_streaming.pid'):
            os.remove('/tmp/spark_streaming.pid')
        
        print("âœ… Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ñ‹")
        
    except Exception as e:
        print(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸: {e}")

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ DAG
with DAG(
    'full_movie_analytics_pipeline',
    default_args=default_args,
    description='ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸ÐºÐ¸ Ñ„Ð¸Ð»ÑŒÐ¼Ð¾Ð² Ñ Kafka Ð¸ Spark',
    schedule_interval=timedelta(hours=2),  # Ð—Ð°Ð¿ÑƒÑÐº ÐºÐ°Ð¶Ð´Ñ‹Ðµ 2 Ñ‡Ð°ÑÐ°
    catchup=False,
    tags=['movie', 'kafka', 'spark', 'analytics', 'etl']
) as dag:

    health_check = PythonOperator(
        task_id='check_services_health',
        python_callable=check_services_health
    )

    kafka_producer = PythonOperator(
        task_id='start_kafka_producer',
        python_callable=start_kafka_producer
    )

    spark_streaming = PythonOperator(
        task_id='start_spark_streaming',
        python_callable=start_spark_streaming
    )

    verify_data = PythonOperator(
        task_id='verify_processed_data',
        python_callable=verify_processed_data
    )

    update_metrics = PythonOperator(
        task_id='update_business_metrics',
        python_callable=update_business_metrics
    )

    stop_spark = PythonOperator(
        task_id='stop_spark_streaming',
        python_callable=stop_spark_streaming,
        trigger_rule='all_done'  # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ÑÑ Ð²ÑÐµÐ³Ð´Ð°, Ð´Ð°Ð¶Ðµ Ð¿Ñ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐ°Ñ…
    )

    cleanup = PythonOperator(
        task_id='cleanup_temp_data',
        python_callable=cleanup_temp_data,
        trigger_rule='all_done'
    )

    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ
    health_check >> kafka_producer >> spark_streaming >> verify_data >> update_metrics
    spark_streaming >> stop_spark  # ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾ Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¾Ð¹ Ð´Ð°Ð½Ð½Ñ‹Ñ…
    [update_metrics, stop_spark] >> cleanup