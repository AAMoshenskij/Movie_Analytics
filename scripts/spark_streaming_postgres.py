#!/usr/bin/env python3
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import time

def create_spark_session():
    """–°–æ–∑–¥–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é Spark —Å–µ—Å—Å–∏—é"""
    return SparkSession.builder \
        .appName("MovieAnalyticsStreamingPostgres") \
        .master("local[2]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.5.0") \
        .config("spark.streaming.stopGracefullyOnShutdown", "true") \
        .getOrCreate()

def create_kafka_stream(spark):
    """–°–æ–∑–¥–∞–µ—Ç –ø–æ—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Kafka"""
    
    print("üì° –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Kafka...")
    
    schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("movie_id", StringType(), True),
        StructField("duration_seconds", IntegerType(), True),
        StructField("event_type", StringType(), True),
        StructField("device", StringType(), True),
        StructField("timestamp", StringType(), True)
    ])
    
    df = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "user_views_topic") \
        .option("startingOffsets", "latest") \
        .option("failOnDataLoss", "false") \
        .load()
    
    print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Kafka —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    
    parsed_df = df.select(
        from_json(col("value").cast("string"), schema).alias("data")
    ).select("data.*")
    
    return parsed_df

def process_streaming_data(stream_df):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Ç–æ–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ"""
    print("üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
    
    processed_df = stream_df.withColumn(
        "watch_category",
        when(col("duration_seconds") < 300, "short")
        .when((col("duration_seconds") >= 300) & (col("duration_seconds") < 1800), "medium")
        .otherwise("long")
    ).withColumn(
        "processing_timestamp", current_timestamp()
    ).withColumn(
        "original_timestamp", to_timestamp(col("timestamp"))
    )
    
    return processed_df

def write_processed_to_postgres(batch_df, batch_id):
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ PostgreSQL"""
    if batch_df.count() > 0:
        print(f"üíæ –ó–∞–ø–∏—Å—ã–≤–∞–µ–º {batch_df.count()} –∑–∞–ø–∏—Å–µ–π –≤ user_views_processed...")
        
        batch_df.select(
            "user_id", "movie_id", "duration_seconds", "event_type",
            "device", "original_timestamp", "watch_category", "processing_timestamp"
        ).write \
            .format("jdbc") \
            .option("url", "jdbc:postgresql://localhost:5432/analytics") \
            .option("dbtable", "user_views_processed") \
            .option("user", "admin") \
            .option("password", "password") \
            .option("driver", "org.postgresql.Driver") \
            .mode("append") \
            .save()
        
        print(f"‚úÖ –ë–∞—Ç—á {batch_id} –∑–∞–ø–∏—Å–∞–Ω –≤ PostgreSQL")

def write_movie_stats_to_postgres(batch_df, batch_id):
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ñ–∏–ª—å–º–∞–º –≤ PostgreSQL"""
    if batch_df.count() > 0:
        print(f"üìä –ó–∞–ø–∏—Å—ã–≤–∞–µ–º {batch_df.count()} –∞–≥—Ä–µ–≥–∞—Ü–∏–π movie_stats...")
        
        batch_df.select(
            col("window.start").alias("window_start"),
            col("window.end").alias("window_end"),
            "movie_id", "view_count", "total_watch_time", "avg_watch_time"
        ).write \
            .format("jdbc") \
            .option("url", "jdbc:postgresql://localhost:5432/analytics") \
            .option("dbtable", "movie_stats_realtime") \
            .option("user", "admin") \
            .option("password", "password") \
            .option("driver", "org.postgresql.Driver") \
            .mode("append") \
            .save()

def write_device_stats_to_postgres(batch_df, batch_id):
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º –≤ PostgreSQL"""
    if batch_df.count() > 0:
        print(f"üì± –ó–∞–ø–∏—Å—ã–≤–∞–µ–º {batch_df.count()} –∞–≥—Ä–µ–≥–∞—Ü–∏–π device_stats...")
        
        batch_df.select(
            col("window.start").alias("window_start"),
            col("window.end").alias("window_end"),
            "device", "view_count", "avg_watch_time"
        ).write \
            .format("jdbc") \
            .option("url", "jdbc:postgresql://localhost:5432/analytics") \
            .option("dbtable", "device_stats_realtime") \
            .option("user", "admin") \
            .option("password", "password") \
            .option("driver", "org.postgresql.Driver") \
            .mode("append") \
            .save()

def create_movie_aggregations(stream_df):
    """–°–æ–∑–¥–∞–µ—Ç –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ —Ñ–∏–ª—å–º–∞–º"""
    return stream_df \
        .withWatermark("processing_timestamp", "1 minute") \
        .groupBy(
            window(col("processing_timestamp"), "1 minute"),
            col("movie_id")
        ) \
        .agg(
            count("user_id").alias("view_count"),
            sum("duration_seconds").alias("total_watch_time"),
            avg("duration_seconds").alias("avg_watch_time")
        )

def create_device_aggregations(stream_df):
    """–°–æ–∑–¥–∞–µ—Ç –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º"""
    return stream_df \
        .withWatermark("processing_timestamp", "1 minute") \
        .groupBy(
            window(col("processing_timestamp"), "1 minute"),
            col("device")
        ) \
        .agg(
            count("user_id").alias("view_count"),
            avg("duration_seconds").alias("avg_watch_time")
        )

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Spark Streaming —Å –∑–∞–ø–∏—Å—å—é –≤ PostgreSQL"""
    print("üöÄ –ó–∞–ø—É—Å–∫ Spark Streaming ETL —Å –∑–∞–ø–∏—Å—å—é –≤ PostgreSQL...")
    print("   PostgreSQL: localhost:5432/analytics")
    
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Ç–æ–∫ –∏–∑ Kafka
        kafka_stream = create_kafka_stream(spark)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        processed_stream = process_streaming_data(kafka_stream)
        
        # –°–æ–∑–¥–∞–µ–º –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
        movie_aggregations = create_movie_aggregations(processed_stream)
        device_aggregations = create_device_aggregations(processed_stream)
        
        print("‚úÖ –í—Å–µ –≥–æ—Ç–æ–≤–æ! –ó–∞–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–∏–º–∏–Ω–≥...")
        print("   –î–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å—Å—è –≤ PostgreSQL –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏")
        print("   –ù–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
        print("-" * 50)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ PostgreSQL
        processed_query = processed_stream.writeStream \
            .foreachBatch(write_processed_to_postgres) \
            .outputMode("append") \
            .option("checkpointLocation", "/tmp/checkpoint-processed") \
            .start()
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —Ñ–∏–ª—å–º–æ–≤ –≤ PostgreSQL
        movie_query = movie_aggregations.writeStream \
            .foreachBatch(write_movie_stats_to_postgres) \
            .outputMode("update") \
            .option("checkpointLocation", "/tmp/checkpoint-movies") \
            .start()
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –≤ PostgreSQL
        device_query = device_aggregations.writeStream \
            .foreachBatch(write_device_stats_to_postgres) \
            .outputMode("update") \
            .option("checkpointLocation", "/tmp/checkpoint-devices") \
            .start()
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        processed_query.awaitTermination()
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Spark Streaming...")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
    finally:
        spark.stop()
        print("üîö Spark —Å–µ—Å—Å–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

if __name__ == "__main__":
    main()